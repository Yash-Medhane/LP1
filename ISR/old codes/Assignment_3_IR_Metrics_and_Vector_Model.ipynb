{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12124,
     "status": "ok",
     "timestamp": 1755075557811,
     "user": {
      "displayName": "Tanisha Kayasth",
      "userId": "00297033920556732523"
     },
     "user_tz": -330
    },
    "id": "cPH9D3XVphuE",
    "outputId": "3c6bf667-b947-4649-e0a4-839542167fc0"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number of documents for clustering:  3\n",
      "Enter document 1 name (without .txt):  doc1\n",
      "Enter document 2 name (without .txt):  doc2\n",
      "Enter document 3 name (without .txt):  doc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consider D2,\n",
      "Sim(D2, C1) = 0.00\n",
      "Smax(D2, C1) = 0.00\n",
      "New cluster created\n",
      "\n",
      "Consider D3,\n",
      "Sim(D3, C1) = 0.00\n",
      "Sim(D3, C2) = 0.00\n",
      "Smax(D3, C1) = 0.00\n",
      "New cluster created\n",
      "\n",
      "Final Clusters:\n",
      "C1 = { D1 }\n",
      "C2 = { D2 }\n",
      "C3 = { D3 }\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def read_keywords(input):\n",
    "    keywords = []\n",
    "    doc_vectors = []\n",
    "\n",
    "    # First pass: collect unique keywords\n",
    "    for fname in input:\n",
    "        with open(fname, 'r') as f:\n",
    "            words = []\n",
    "            while True:\n",
    "                token = f.readline()\n",
    "                if not token:\n",
    "                    break\n",
    "                parts = token.strip().split()\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                word, freq = parts[0], int(parts[1])\n",
    "                words.append((word, freq))\n",
    "                if word not in keywords:\n",
    "                    keywords.append(word)\n",
    "            doc_vectors.append(words)\n",
    "\n",
    "    # Second pass: build document vectors in fixed keyword order\n",
    "    vectors = []\n",
    "    for words in doc_vectors:\n",
    "        vec = [0] * len(keywords)\n",
    "        for word, freq in words:\n",
    "            idx = keywords.index(word)\n",
    "            vec[idx] = freq\n",
    "        vectors.append(vec)\n",
    "\n",
    "    return keywords, vectors\n",
    "\n",
    "\n",
    "def sim(vec1, vec2):\n",
    "    num = 2 * sum(a * b for a, b in zip(vec1, vec2))\n",
    "    den = sum(a ** 2 for a in vec1) + sum(b ** 2 for b in vec2)\n",
    "    return num / den if den != 0 else 0\n",
    "\n",
    "\n",
    "def single_pass_clustering(vectors, threshold=0.6):\n",
    "    clusters = [[0]]  # first document in first cluster\n",
    "    centroids = [vectors[0][:]]\n",
    "\n",
    "    for doc_id in range(1, len(vectors)):\n",
    "        sims = [sim(vectors[doc_id], centroid) for centroid in centroids]\n",
    "        max_sim = max(sims)\n",
    "        best_cluster = sims.index(max_sim)\n",
    "\n",
    "        print(f\"\\nConsider D{doc_id+1},\")\n",
    "        for i, s in enumerate(sims):\n",
    "            print(f\"Sim(D{doc_id+1}, C{i+1}) = {s:.2f}\")\n",
    "\n",
    "        if max_sim > threshold:\n",
    "            print(f\"Smax(D{doc_id+1}, C{best_cluster+1}) = {max_sim:.2f}\")\n",
    "            clusters[best_cluster].append(doc_id)\n",
    "            # update centroid\n",
    "            centroids[best_cluster] = [\n",
    "                (a + b) / 2 for a, b in zip(centroids[best_cluster], vectors[doc_id])\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Smax(D{doc_id+1}, C{best_cluster+1}) = {max_sim:.2f}\")\n",
    "            print(\"New cluster created\")\n",
    "            clusters.append([doc_id])\n",
    "            centroids.append(vectors[doc_id][:])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    no = int(input(\"Enter number of documents for clustering: \"))\n",
    "    filenames = [input(f\"Enter document {i+1} name (without .txt): \") + \".txt\" for i in range(no)]\n",
    "\n",
    "    if no <= 1:\n",
    "        print(\"Number of documents should be greater than 1!\")\n",
    "        exit(0)\n",
    "\n",
    "    keywords, vectors = read_keywords(filenames)\n",
    "    clusters = single_pass_clustering(vectors)\n",
    "\n",
    "    print(\"\\nFinal Clusters:\")\n",
    "    for idx, cluster in enumerate(clusters, start=1):\n",
    "        members = \" \".join(f\"D{doc_id+1}\" for doc_id in cluster)\n",
    "        print(f\"C{idx} = {{ {members} }}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOE7NFx1pqKb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOB7S/0TRxb1+sWL06el2QK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
