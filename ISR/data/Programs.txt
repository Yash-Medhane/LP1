Conflation : 

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string


# Download resources (run once)
# nltk.download('punkt')
# nltk.download('punkt_tab')   # <-- ADD THIS LINE
# nltk.download('stopwords')
# nltk.download('wordnet')

# Step 1: Read input text file
filename = "input.txt"   # <-- use your own text file
with open(filename, 'r', encoding='utf-8') as file:
    text = file.read()

print("Original Text:\n", text)
print("-" * 80)

# Step 2: Tokenization
tokens = word_tokenize(text.lower())

# Step 3: Remove punctuation and stopwords
stop_words = set(stopwords.words('english'))
tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

# Step 4: Apply Stemming
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in tokens]

# Step 5: Apply Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]

# Step 6: Display results
print("After Stemming:\n", ' '.join(stemmed_words))
print("-" * 80)
print("After Lemmatization:\n", ' '.join(lemmatized_words))
print("-" * 80)

# Step 7: Create Document Representative (word frequency)
from collections import Counter
freq = Counter(stemmed_words)
print("Document Representative (Word Frequency):")
for word, count in freq.most_common(10):
    print(f"{word}: {count}")

---------------------------------------------------------------------------------------------------------------------
Single Pass Clustering 

```python
def sim(v1, v2):
    dot = sum(a*b for a, b in zip(v1, v2))
    mag1 = sum(a*a for a in v1) ** 0.5
    mag2 = sum(b*b for b in v2) ** 0.5
    return dot / (mag1 * mag2) if mag1 and mag2 else 0
```


```python
docs = [
    [1, 2, 0, 0],   # D1
    [1, 1, 0, 0],   # D2  -> similar to D1
    [0, 0, 5, 4],   # D3
    [0, 0, 4, 5]    # D4  -> similar to D3
]
```


```python
threshold = 0.8
clusters = [[0]]
centroids = [docs[0]]

for i in range(1, len(docs)):
    sims = [sim(docs[i], c) for c in centroids]
    max_sim = max(sims)
    j = sims.index(max_sim)

    if max_sim >= threshold:
        clusters[j].append(i)
    else:
        clusters.append([i])
        centroids.append(docs[i])
```


```python
print("\nFinal Clusters:")
for i, c in enumerate(clusters, 1):
    print(f"C{i}: ", [f"D{d+1}" for d in c])
```

    
    Final Clusters:
    C1:  ['D1', 'D2']
    C2:  ['D3', 'D4']
    


```python
-------------------------------------------------------------------------------------------------------------
Inverted Index

```python
import os, time
from whoosh.fields import Schema, TEXT, ID
from whoosh.index import create_in
from whoosh.qparser import QueryParser, AndGroup, OrGroup
```


```python
# ---------- Step 1: Schema & index ----------
schema = Schema(title=ID(stored=True), content=TEXT)
index_dir = f"indexdir_{int(time.time())}"
os.mkdir(index_dir)
ix = create_in(index_dir, schema)
```


```python
# ---------- Step 2: Add documents ----------
writer = ix.writer()
writer.add_document(title="doc1.txt", content=open("doc1.txt").read())
writer.add_document(title="doc2.txt", content=open("doc2.txt").read())
writer.commit()
print("Documents indexed successfully!\n")
```

    Documents indexed successfully!
    
    


```python
# ---------- Step 3: Search function ----------
def search_docs(query_str, use_or=False):
    with ix.searcher() as searcher:
        group = OrGroup.factory(0.9) if use_or else AndGroup
        qp = QueryParser("content", ix.schema, group=group)
        query = qp.parse(query_str)
        results = searcher.search(query)

        print(f"Results for query: '{query_str}'")
        if results:
            for r in results:
                print("-", r["title"])
        else:
            print("No matching documents found.")
        print()
```


```python
# ---------- Step 4: Run Boolean search ----------
search_docs("python AND web")       # AND query
search_docs("python OR web", True)  # OR query
```
------------------------------------------------------------------------------------------------------
Evaluation 

```python
# Query: "python data"
retrieved = ["doc1.txt", "doc2.txt", "doc3.txt"]  # system's ranked output
relevant = ["doc1.txt", "doc2.txt"]               # ground truth
```


```python
import math

# True Positives, False Positives, False Negatives
tp = len([d for d in retrieved if d in relevant])
fp = len([d for d in retrieved if d not in relevant])
fn = len([d for d in relevant if d not in retrieved])

# Precision, Recall
precision = tp / (tp + fp)
recall = tp / (tp + fn)

# F1 and E-Measure
f1 = 2 * precision * recall / (precision + recall)
e_measure = 1 - f1  # complement (simplified)
```


```python
# NDCG Calculation
relevance_scores = [1 if d in relevant else 0 for d in retrieved]

def dcg(scores):
    return sum(s / math.log2(i + 2) for i, s in enumerate(scores))

ideal = sorted(relevance_scores, reverse=True)
ndcg = dcg(relevance_scores) / dcg(ideal) if dcg(ideal) != 0 else 0

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Measure: {f1:.2f}")
print(f"E-Measure: {e_measure:.2f}")
print(f"NDCG: {ndcg:.2f}")
---------------------------------------------------------------------------------------------------
Weather & Web Scrapper

import requests

city = input("Enter city name: ")

api_key = "bcb62df34de91d9f9387459111df3d4c"  # replace with your own key
url = f"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
response = requests.get(url)
data = response.json()

if response.status_code == 200:
    print("City:", data["name"])
    print("Temperature:", data["main"]["temp"], "Â°C")
    print("Wind Speed:", data["wind"]["speed"], "m/s")
    print("Description:", data["weather"][0]["description"])
else:
    print("City not found or API error!")
----------------------------------------------------------------------------------------------
Web Crawler : 

import requests, time
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin

base = "https://books.toscrape.com/catalogue/page-1.html"
headers = {"User-Agent": "SimpleCrawler"}
data, url = [], base

for _ in range(2):  # crawl 5 pages
    print("Crawling:", url)
    soup = BeautifulSoup(requests.get(url, headers=headers).text, "html.parser")

    for b in soup.select(".product_pod"):
        data.append({
            "title": b.h3.a["title"],
            "price": b.select_one(".price_color").text,
            "availability": b.select_one(".availability").text.strip(),
            "link": urljoin(url, b.h3.a["href"])
        })

    nxt = soup.select_one(".next a")
    if not nxt: break
    url = urljoin(url, nxt["href"])
    time.sleep(1)

import pandas as pd
pd.DataFrame(data).to_csv("bookks.csv", index=False)
print("Saved!")
-----------------------------------------------------------------------------------------

Input.txt

The quick brown fox is jumping over the lazy dogs.
Dogs are friendly and playful animals.
The foxes were running swiftly in the forest.
Happiness comes from helping others and being kind.
Learning coding and reading books are fulfilling activities.

-------------------------------------------------------------------------------------------

Doc1.txt

Python is a powerful programming language.
It is widely used in web development and data analysis.

------------------------------------------------------------------------------------------

Doc2.txt

Learning Python improves problem solving and logical thinking.
Data analysis with Python is efficient and fun.

-----------------------------------------------------------------------------------------

Doc3.txt

Machine learning models improve data-driven decisions.

-----------------------------------------------------------------------------------------

import cv2
import numpy as np
import matplotlib.pyplot as plt

filename = "image.jpeg"

img = cv2.imread(filename)

if img is None:
    print("Image not found !")
else:
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    # display image
    plt.figure(figsize=(5,5))
    plt.imshow(img)
    plt.axis('off')
    plt.show()

colors = ('r','g','b')
plt.figure(figsize=(8,5))
for i,color in enumerate(colors):
    hist = cv2.calcHist([img],[i],None,[256],[0,256])
    plt.plot(hist,color=color)
    plt.xlim([0,256])
plt.title("Color Histogram")
plt.xlabel("pixel intensity")
plt.ylabel("frequency")

mean_colors = cv2.mean(img)[:3]
print(f"R value : {mean_colors[0]:.2f}")
print(f"G value : {mean_colors[1]:.2f}")
print(f"B value : {mean_colors[2]:.2f}")
